name: Build Test Packages

on:
  workflow_dispatch:
    inputs:
      package_ids:
        description: 'Package IDs to build (e.g., "1-10,12,19"). Max 120 IDs. Leave empty for default (1-5)'
        required: false
        default: ''
        type: string
      use_cached_fetchers:
        description: 'Use cached fetchers for package requests'
        required: false
        default: true
        type: boolean
      randomize:
        description: 'Randomize package selection'
        required: false
        default: false
        type: boolean
      model:
        description: 'Model to use (e.g., anthropic/claude-3-5-haiku-20241022, ollama/qwen2.5-coder:32b, gemini/gemini-2.5-flash, or openai/gpt-4.1-mini-2025-04-14)'
        required: false
        default: 'gemini/gemini-2.5-flash'
        type: string
      vibenix_settings:
        description: 'Vibenix settings as JSON (e.g., {"tools": ["get_builder_packages"]})'
        required: false
        type: string
      model_settings:
        description: 'Model settings as JSON (e.g., {"temperature": 0.0, "max_tokens": 32768})'
        required: false
        type: string
      ollama_host:
        description: 'Ollama host URL (e.g., http://your-host.tailnet:11434)'
        required: false
        type: string
      dataset_manual:
        description: 'Dataset directory/file path. URL-based: packaging_requests. CSV-based: single_fetcher_2025-08-01_2025-11-20.csv'
        required: false
        default: 'single_fetcher_2025-08-01_2025-11-20.csv'
        type: string
      csv_file:
        description: 'Which CSV file to use within the dataset'
        required: false
        default: 'implementation'
        type: choice
        options:
          - implementation
          - evaluation
      concurrent_jobs:
        description: 'Number of jobs to run in parallel (1 to avoid rate limits)'
        required: false
        default: '1'
        type: number
  push:
    branches:
      - main
    paths:
      - 'research/packaging_requests/used_during_implementation.csv'
      - 'src/**'
      - '.github/workflows/train-on-dataset.yml'

jobs:
  setup:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.create-matrix.outputs.matrix }}
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: Install Nix
        uses: cachix/install-nix-action@v31
        
      - name: Setup Cachix
        uses: cachix/cachix-action@v15
        with:
          name: vibenix
          authToken: '${{ secrets.CACHIX_TOKEN }}'
        
      - name: Build vibenix environment
        run: |
          nix develop .# --command echo "Development environment ready"

      - name: Generate vibenix cache
        run: |
          nix develop -c python -c '''
          from vibenix.tools.search_related_packages import _get_builder_functions
          from vibenix import config
          from vibenix.flake import init_flake
          config.init()
          init_flake()
          _get_builder_functions()'''

      - name: Upload cache artifact
        uses: actions/upload-artifact@v4
        with:
          name: vibenix-cache
          path: cachedir/
        
      - name: Create matrix from CSV
        id: create-matrix
        run: |
          python3 -c "
          import csv
          import json
          import random
          import os
          import sys

          # Add scripts directory to path to import id_range_parser
          sys.path.insert(0, 'scripts')
          from id_range_parser import parse_id_ranges, validate_id_range_constraints

          matrix_data = []
          dataset_path = '${{ github.event.inputs.dataset_manual || 'packaging_requests' }}'

          # Detect dataset type: if it ends with .csv, it's the new CSV format
          if dataset_path.endswith('.csv'):
              # New CSV-based format with pname, version, fetcher columns and random_order ID
              csv_file = f'research/{dataset_path}'

              # Validate use_cached_fetchers is true for CSV-based datasets
              use_cached = '${{ github.event.inputs.use_cached_fetchers || 'true' }}' == 'true'
              if not use_cached:
                  raise ValueError('CSV-based datasets require use_cached_fetchers to be true')

              # Read all rows indexed by ID
              rows_by_id = {}
              with open(csv_file, 'r') as f:
                  reader = csv.DictReader(f)
                  for row in reader:
                      if row['package_name']:  # Skip entries without package names
                          try:
                              row_id = int(row['random_order'])
                              rows_by_id[row_id] = row
                          except (ValueError, KeyError):
                              print(f'Warning: Skipping row without valid random_order ID', file=sys.stderr)

              # Parse package ID ranges
              package_ids_input = '${{ github.event.inputs.package_ids || '' }}'.strip()

              if package_ids_input:
                  # Parse the ID range specification
                  try:
                      selected_ids = parse_id_ranges(package_ids_input)
                      # Validate constraints (max 120 IDs, valid range 1-532)
                      validate_id_range_constraints(selected_ids, max_count=120, valid_range=(1, 532))
                  except ValueError as e:
                      raise ValueError(f'Invalid package_ids input: {e}')
              else:
                  # Default: IDs 1-5
                  selected_ids = [1, 2, 3, 4, 5]

              # Build matrix data from selected IDs
              for row_id in selected_ids:
                  if row_id in rows_by_id:
                      row = rows_by_id[row_id]
                      matrix_data.append({
                          'package_name': row['package_name'],
                          'pname': row['pname'],
                          'version': row['version'],
                          'id': row_id,
                          'dataset_type': 'csv-based',
                          'csv_dataset_path': csv_file
                      })
                  else:
                      print(f'Warning: ID {row_id} not found in dataset', file=sys.stderr)

          else:
              # Original URL-based format from directory (legacy - doesn't use IDs)
              dataset_dir = f'research/{dataset_path}'

              # Select CSV file based on csv_file choice
              csv_file_choice = '${{ github.event.inputs.csv_file || 'implementation' }}'
              if csv_file_choice == 'implementation':
                  csv_file = f'{dataset_dir}/used_during_implementation.csv'
              else:  # evaluation
                  csv_file = f'{dataset_dir}/post_project_evaluation.csv'

              with open(csv_file, 'r') as f:
                  reader = csv.DictReader(f)
                  for row in reader:
                      if row['repo_url']:  # Skip entries without repo URLs
                          clean_name = row['repo_url'].replace('https://github.com/', '')
                          matrix_data.append({
                              'issue_number': row['issue_number'],
                              'repo_url': row['repo_url'],
                              'repo_name': clean_name,
                              'dataset_type': 'url-based',
                              'dataset_dir': dataset_dir
                          })

              # For URL-based datasets, randomize if requested and limit to 5 by default
              randomize = '${{ github.event.inputs.randomize }}' == 'true'
              if randomize:
                  random.shuffle(matrix_data)

              # Limit to 5 for URL-based (legacy behavior)
              matrix_data = matrix_data[:5]

          matrix = {'include': matrix_data}

          with open('matrix.json', 'w') as f:
              json.dump(matrix, f)

          # Set output for GitHub Actions
          print(f'matrix={json.dumps(matrix)}')
          "
          echo "matrix=$(cat matrix.json)" >> $GITHUB_OUTPUT

  build-test-packages:
    needs: setup
    runs-on: ubuntu-latest
    name: Package ${{ matrix.repo_name || matrix.package_name }}
    timeout-minutes: 45
    strategy:
      matrix: ${{ fromJson(needs.setup.outputs.matrix) }}
      max-parallel: ${{ fromJson(github.event.inputs.concurrent_jobs || '1') }}  # Configurable parallel jobs
      fail-fast: false
    steps:
      - uses: actions/checkout@v4
      - uses: actions/download-artifact@v4
        with:
          name: vibenix-cache
          path: cachedir/
      
      - name: Set provider
        id: provider
        run: |
          # Determine provider from model input
          model="${{ github.event.inputs.model || 'gemini/gemini-2.5-flash' }}"
          if [[ "$model" == ollama/* ]]; then
            provider="ollama"
          elif [[ "$model" == openai/* || "$model" == gpt-* ]]; then
            provider="openai"
          elif [[ "$model" == anthropic/* || "$model" == claude-* ]]; then
            provider="anthropic"
          elif [[ "$model" == gemini/* || "$model" == *gemini* ]]; then
            provider="gemini"
          else
            provider="openai"  # Default to openai if unknown
          fi
          echo "provider=$provider" >> $GITHUB_OUTPUT
      
      - name: Install Nix
        uses: cachix/install-nix-action@v31
        
      - name: Setup Cachix
        uses: cachix/cachix-action@v15
        with:
          name: vibenix
          authToken: '${{ secrets.CACHIX_TOKEN }}'
        
      - name: Tailscale
        if: ${{ steps.provider.outputs.provider == 'ollama' || steps.provider.outputs.provider == 'openai' }}
        uses: tailscale/github-action@v3
        with:
          oauth-client-id: ${{ secrets.TS_OAUTH_CLIENT_ID }}
          oauth-secret: ${{ secrets.TS_OAUTH_SECRET }}
          tags: tag:oauth-client-gh-action-hydralisk-ollama
          
      - name: Create vibenix config
        run: |
          mkdir -p ~/.vibenix
          
          # Convert ollama/modelname to openai/modelname (no-op for other providers)
          model="${{ github.event.inputs.model || 'gemini/gemini-2.5-flash' }}"
          model_converted=$(echo "$model" | sed 's|^ollama/|openai/|')
          
          # Use openai provider if ollama was selected
          provider="${{ steps.provider.outputs.provider }}"
          if [ "$provider" = "ollama" ]; then
            provider="openai"
          fi
          
          # Build config JSON dynamically
          config_json='{"provider": "'$provider'", "model": "'$model_converted'", "backend": "litellm"'
          
          # Add ollama_host and openai_api_base only if ollama_host input is provided
          if [ -n "${{ github.event.inputs.ollama_host }}" ]; then
            config_json="${config_json}, \"ollama_host\": \"${{ github.event.inputs.ollama_host }}\""
            config_json="${config_json}, \"openai_api_base\": \"${{ github.event.inputs.ollama_host }}/v1\""
          fi
          
          config_json="${config_json}}"
          
          echo "$config_json" > ~/.vibenix/config.json
          
      - name: Create vibenix settings
        run: |
          VIBENIX_SETTINGS='''${{ github.event.inputs.vibenix_settings }}'''
          # Set vibenix settings from input into config file (only if provided)
          if [ -n "$VIBENIX_SETTINGS" ]; then
            mkdir -p ~/.vibenix
            echo '''${{ github.event.inputs.vibenix_settings }}''' > ~/.vibenix/vibenix_settings.json
          fi

      - name: Setup environment
        run: |
          # Set API keys as environment variables based on provider
          if [ "${{ steps.provider.outputs.provider }}" = "anthropic" ]; then
            echo "ANTHROPIC_API_KEY=${{ secrets.ANTHROPIC_API_KEY }}" >> $GITHUB_ENV
          elif [ "${{ steps.provider.outputs.provider }}" = "gemini" ]; then
            echo "GEMINI_API_KEY=${{ secrets.GEMINI_API_KEY }}" >> $GITHUB_ENV
          elif [ "${{ steps.provider.outputs.provider }}" = "ollama" ]; then
            # For Ollama via OpenAI endpoint, set a dummy API key if needed
            echo "OPENAI_API_KEY=dummy" >> $GITHUB_ENV
          elif [ "${{ steps.provider.outputs.provider }}" = "openai" ]; then
            echo "OPENAI_API_KEY=${{ secrets.OPENAI_API_KEY }}" >> $GITHUB_ENV
          fi

          # Set model settings from input into env (only if provided)
          if [ -n "${{ github.event.inputs.model_settings }}" ]; then
            echo "VIBENIX_MODEL_SETTINGS='${{ github.event.inputs.model_settings }}'" >> $GITHUB_ENV
          fi

          # Set Github token in nix config
          echo "NIX_CONFIG='access-tokens = github.com=${{ secrets.GH_TOKEN }}'" >> $GITHUB_ENV
      
      - name: Process package request
        id: process
        run: |
          # Determine dataset type and set appropriate variables
          if [ "${{ matrix.dataset_type }}" = "csv-based" ]; then
            echo "Processing CSV-based package: ${{ matrix.package_name }}"
            identifier="${{ matrix.package_name }}"
            log_file="${identifier}.log"

            # CSV-based datasets always use the fetcher from CSV
            vibenix_args="--csv-dataset ${{ matrix.csv_dataset_path }} --csv-package ${{ matrix.package_name }}"
          else
            echo "Processing URL-based issue #${{ matrix.issue_number }} - ${{ matrix.repo_url }}"
            identifier="${{ matrix.issue_number }}"
            log_file="${identifier}.log"

            # Prepare fetcher argument if use_cached_fetchers is true
            fetcher_arg=""
            if [ "${{ github.event.inputs.use_cached_fetchers || 'true' }}" = "true" ]; then
              # Extract repo owner and name from URL
              repo_path=$(echo "${{ matrix.repo_url }}" | sed 's|https://github.com/||')
              repo_owner=$(echo "$repo_path" | cut -d'/' -f1)
              repo_name=$(echo "$repo_path" | cut -d'/' -f2)
              fetcher_file="${{ matrix.dataset_dir }}/fetchers/${repo_owner},${repo_name}.nix"

              # Check if fetcher file exists
              if [ -f "$fetcher_file" ]; then
                fetcher_arg="--fetcher $fetcher_file"
                echo "Using cached fetcher: $fetcher_file"
              else
                echo "Cached fetcher not found: $fetcher_file"
              fi
            fi

            vibenix_args="$fetcher_arg ${{ matrix.repo_url }}"
          fi

          # Set default status
          echo "status=failed" >> $GITHUB_OUTPUT

          # Create output directory for this run
          mkdir -p output

          # Run vibenix with raw output and save results
          set -o pipefail
          nix develop .# -c python -m vibenix \
            --raw \
            --output-dir output \
            $vibenix_args \
            2>&1 | tee "$log_file"
          exit_code=$?
          
          # Check if the command succeeded AND produced a package.nix file
          if [ $exit_code -eq 0 ]; then
            # Command didn't fail, but we need to verify it actually created a package
            if [ -d output ]; then
              # Find any package.nix file in subdirectories
              package_nix_path=$(find output -name "package.nix" -type f | head -n1)
              if [ -n "$package_nix_path" ]; then
                # Extract package name from path (e.g., output/wikiman/package.nix -> wikiman)
                package_name=$(basename $(dirname "$package_nix_path"))
                echo "status=success" >> $GITHUB_OUTPUT
                echo "package_name=$package_name" >> $GITHUB_OUTPUT
              else
                echo "status=failed" >> $GITHUB_OUTPUT
                echo "Command succeeded but no package.nix was generated"
                exit 1  # Fail the step
              fi
            else
              echo "status=failed" >> $GITHUB_OUTPUT
              echo "Command succeeded but no output directory was created"
              exit 1  # Fail the step
            fi
          else
            echo "status=failed" >> $GITHUB_OUTPUT
            echo "Exit code: $exit_code"
            echo "=== Combined log ==="
            head -n 50 "$log_file" || echo "No log found"
            exit $exit_code  # Propagate the original exit code
          fi

      - name: Prepare artifact directory
        if: always()
        run: |
          # Determine identifier based on dataset type
          if [ "${{ matrix.dataset_type }}" = "csv-based" ]; then
            identifier="${{ matrix.package_name }}"
            name_sanitized="${{ matrix.pname }}"
          else
            identifier="${{ matrix.issue_number }}"
            name_sanitized=$(echo "${{ matrix.repo_name }}" | tr '/' '-')
          fi

          mkdir -p artifact-${identifier}
          cp ${identifier}.log artifact-${identifier}/ 2>/dev/null || true
          if [ -d output ]; then
            cp -r output/* artifact-${identifier}/
          fi
          cp ~/.vibenix/config.json artifact-${identifier}/ 2>/dev/null || true

          # Create artifact name
          echo "artifact_name=vibenix-output-${identifier}-${name_sanitized}" >> $GITHUB_ENV
          echo "identifier=${identifier}" >> $GITHUB_ENV
          
      - name: Upload results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: ${{ env.artifact_name }}
          path: artifact-${{ env.identifier }}/
          retention-days: 1
          
      - name: Create summary
        if: always()
        run: |
          # Set emoji based on status
          if [ "${{ steps.process.outputs.status }}" = "success" ]; then
            status_emoji="✅"
            status_text="SUCCESS"
          else
            status_emoji="❌"
            status_text="FAILED"
          fi

          # Determine identifier and log file based on dataset type
          if [ "${{ matrix.dataset_type }}" = "csv-based" ]; then
            identifier="${{ matrix.package_name }}"
            log_file="${identifier}.log"
            echo "## ${status_emoji} Package: ${{ matrix.pname }}" >> $GITHUB_STEP_SUMMARY
            echo "- Package name: ${{ matrix.package_name }}" >> $GITHUB_STEP_SUMMARY
            echo "- Version: ${{ matrix.version }}" >> $GITHUB_STEP_SUMMARY
          else
            identifier="${{ matrix.issue_number }}"
            log_file="${identifier}.log"
            echo "## ${status_emoji} Issue #${{ matrix.issue_number }}" >> $GITHUB_STEP_SUMMARY
            echo "- Repository: ${{ matrix.repo_url }}" >> $GITHUB_STEP_SUMMARY
            echo "- Packaging Request: https://github.com/NixOS/nixpkgs/issues/${{ matrix.issue_number }}" >> $GITHUB_STEP_SUMMARY
          fi

          echo "- Provider: ${{ steps.provider.outputs.provider }}" >> $GITHUB_STEP_SUMMARY
          echo "- Model: ${{ github.event.inputs.model || 'gemini/gemini-2.5-flash' }}" >> $GITHUB_STEP_SUMMARY
          echo "- Status: ${status_emoji} **${status_text}**" >> $GITHUB_STEP_SUMMARY
          if [ -n "${{ steps.process.outputs.package_name }}" ]; then
            echo "- Generated package name: ${{ steps.process.outputs.package_name }}" >> $GITHUB_STEP_SUMMARY
          fi
          if [ -f "$log_file" ]; then
            echo "- Log size: $(wc -c < $log_file) bytes" >> $GITHUB_STEP_SUMMARY
          fi
          if [ -f "output/${{ steps.process.outputs.package_name }}/package.nix" ]; then
            echo "- Generated package.nix: ✓" >> $GITHUB_STEP_SUMMARY
          fi
          if [ -f "output/run.ccl" ]; then
            echo "- Generated run.ccl: ✓" >> $GITHUB_STEP_SUMMARY
            # Extract total cost from run.ccl if present
            total_cost=$(grep "total_cost =" "output/run.ccl" | tail -1 | cut -d'=' -f2 | tr -d ' ')
            if [ -n "$total_cost" ]; then
              echo "- Total API cost: \$$total_cost" >> $GITHUB_STEP_SUMMARY
            fi
          fi
          if [ -f "$log_file" ] && [ -s "$log_file" ]; then
            echo "### Log excerpt:" >> $GITHUB_STEP_SUMMARY
            echo '<pre>' >> $GITHUB_STEP_SUMMARY
            tail -n 50 "$log_file" >> $GITHUB_STEP_SUMMARY
            echo '</pre>' >> $GITHUB_STEP_SUMMARY
          fi
          
      - name: Fail job if packaging failed
        if: steps.process.outputs.status == 'failed'
        run: |
          echo "Packaging failed, failing the job"
          echo "Status was: ${{ steps.process.outputs.status }}"
          exit 1

  aggregate-results:
    needs: build-test-packages
    runs-on: ubuntu-latest
    if: always()
    steps:
      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: artifacts
          pattern: vibenix-output-*
          
      - name: Aggregate results
        run: |
          echo "# Test Package Build Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          total=0
          success=0
          failed=0
          total_cost=0
          
          # Process each artifact directory
          for dir in artifacts/vibenix-output-*; do
            if [ -d "$dir" ]; then
              # Extract issue number from the directory name (format: vibenix-output-ISSUE-PROJECT)
              base_name=$(basename "$dir")
              issue_num=$(echo "$base_name" | sed 's/vibenix-output-//' | cut -d'-' -f1)
              total=$((total + 1))
              
              # Check if there's a package.nix file in any subdirectory
              package_nix_found=$(find "$dir" -name "package.nix" -type f 2>/dev/null | head -1)
              
              if [ -n "$package_nix_found" ]; then
                success=$((success + 1))
              else
                failed=$((failed + 1))
              fi
              
              # Extract cost from run.ccl if it exists
              if [ -f "$dir/run.ccl" ]; then
                cost=$(grep "total_cost =" "$dir/run.ccl" | tail -1 | cut -d'=' -f2 | tr -d ' ')
                if [ -n "$cost" ]; then
                  # Add to total using bc for floating point arithmetic
                  total_cost=$(echo "$total_cost + $cost" | bc)
                fi
              fi
            fi
          done
          
          echo "## Summary:" >> $GITHUB_STEP_SUMMARY
          echo "- Total processed: $total" >> $GITHUB_STEP_SUMMARY
          echo "- Successful: $success" >> $GITHUB_STEP_SUMMARY
          echo "- Failed: $failed" >> $GITHUB_STEP_SUMMARY
          
          if [ $success -gt 0 ]; then
            success_rate=$((success * 100 / total))
            echo "- Success rate: ${success_rate}%" >> $GITHUB_STEP_SUMMARY
          fi
          
          # Show total cost if any
          if [ $(echo "$total_cost > 0" | bc) -eq 1 ]; then
            echo "- Total API cost: \$$total_cost" >> $GITHUB_STEP_SUMMARY
          fi
          
      - name: Create combined artifact
        run: |
          mkdir -p combined-results
          if [ -d artifacts ] && [ "$(ls -A artifacts)" ]; then
            # Process each artifact and reorganize
            for dir in artifacts/vibenix-output-*; do
              if [ -d "$dir" ]; then
                # Extract issue number and project name from the directory name (format: vibenix-output-ISSUE-PROJECT)
                base_name=$(basename "$dir")
                issue_and_project=$(echo "$base_name" | sed 's/vibenix-output-//')
                issue_num=$(echo "$issue_and_project" | cut -d'-' -f1)
                # Get everything after the first dash as the project name
                project_name=$(echo "$issue_and_project" | cut -d'-' -f2-)
                
                # Create folder with issue-project naming
                if [ -n "$project_name" ]; then
                  folder_name="${issue_num}-${project_name}"
                else
                  folder_name="$issue_num"
                fi
                
                # Create issue-specific directory
                mkdir -p "combined-results/$folder_name"
                
                # Copy log file
                if [ -f "$dir/${issue_num}.log" ]; then
                  cp "$dir/${issue_num}.log" "combined-results/$folder_name/"
                fi
                
                # Copy package files (should be flatter now)
                if [ -f "$dir/run.ccl" ]; then
                  cp "$dir/run.ccl" "combined-results/$folder_name/"
                fi
                
                # Copy package directory if it exists
                for pkg_dir in "$dir"/*/; do
                  if [ -d "$pkg_dir" ] && [ -f "$pkg_dir/package.nix" ]; then
                    pkg_name=$(basename "$pkg_dir")
                    mkdir -p "combined-results/$folder_name/$pkg_name"
                    cp "$pkg_dir/package.nix" "combined-results/$folder_name/$pkg_name/"
                  fi
                done
                
                # Copy config.json
                if [ -f "$dir/config.json" ]; then
                  cp "$dir/config.json" "combined-results/$folder_name/"
                fi
              fi
            done
          else
            echo "No artifacts found to combine"
            echo "No successful packages were built" > combined-results/README.txt
          fi
          
      - name: Upload combined results
        uses: actions/upload-artifact@v4
        with:
          name: all-test-results
          path: combined-results/
          retention-days: 90
